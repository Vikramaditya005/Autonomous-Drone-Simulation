!pip install torch nltk pandas tqdm rouge-score
from google.colab import drive
drive.mount('/content/drive')
import os
import torch
import torch.nn as tr
import torch.optim as tp
from torch.utils.data import Dataset,DataLoader
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from collections import Counter
from tqdm import tqdm
nltk.download('punkt_tab')
nltk.download('punkt')
Dataset_Path="/content/drive/MyDrive/cnn_dailymail"
train_df=pd.read_csv(os.path.join(Dataset_Path,"train.csv"))
val_df=pd.read_csv(os.path.join(Dataset_Path,"validation.csv"))
test_df=pd.read_csv(os.path.join(Dataset_Path,"test.csv"))
train_df=train_df[['article','highlights']].dropna()
val_df=val_df[['article','highlights']].dropna()
test_df=test_df[['article','highlights']].dropna()
print("Train size:",len(train_df))
print("Validation size:",len(val_df))
print("Test size:",len(test_df))
Train_Size=25000
Val_Size=5000
train_df=train_df.sample(Train_Size,random_state=42).reset_index(drop=True)
val_df=val_df.sample(Val_Size,random_state=42).reset_index(drop=True)
print("Training samples:",len(train_df))
print("Validation samples:",len(val_df))
embed_size=256
hidden_size=512
batch_size=16
epochs=3
max_article_len=400
max_summary_len=100
train_df
test_df
val_df
class Vocabulary:
    def __init__(self,freq_threshold=2):
        self.itos={0:'<pad>',1:'<sos>',2:'<eos>',3:'<unk>'}
        self.stoi={v: k for k, v in self.itos.items()}
        self.freq_threshold=freq_threshold
    def build_vocab(self,sentences):
        frequencies=Counter()
        idx=4
        for sentence in sentences:
            for word in word_tokenize(sentence.lower()):
                frequencies[word] +=1
                if frequencies[word]==self.freq_threshold:
                    self.stoi[word]=idx
                    self.itos[idx]=word
                    idx +=1
    def numericalize(self,text):
        return [self.stoi.get(word, self.stoi['<unk>']) for word in word_tokenize(text.lower())]
Vocab_Size=5000
vocab = Vocabulary(freq_threshold=2)
vocab.build_vocab(train_df['article'][:Vocab_Size])
vocab.build_vocab(train_df['highlights'][:Vocab_Size])
print("Vocabulary size:",len(vocab.stoi))
class NewsDataset(Dataset):
    def __init__(self, dataframe, vocab):
        self.data=dataframe.reset_index(drop=True)
        self.vocab=vocab
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        article=self.data.iloc[idx]['article']
        summary=self.data.iloc[idx]['highlights']
        article_vec=[self.vocab.stoi['<sos>']] + \
                     self.vocab.numericalize(article)[:max_article_len] + \
                     [self.vocab.stoi['<eos>']]
        summary_vec=[self.vocab.stoi['<sos>']] + \
                      self.vocab.numericalize(summary)[:max_summary_len] + \
                      [self.vocab.stoi['<eos>']]
        return torch.tensor(article_vec), torch.tensor(summary_vec)
from torch.nn.utils.rnn import pad_sequence
def collate_fn(batch):
    articles, summaries=zip(*batch)
    articles_padded=pad_sequence(articles,batch_first=True,padding_value=vocab.stoi['<pad>'])
    summaries_padded = pad_sequence(summaries,batch_first=True,padding_value=vocab.stoi['<pad>'])
    return articles_padded,summaries_padded
train_dataset=NewsDataset(train_df,vocab)
val_dataset=NewsDataset(val_df,vocab)
test_dataset=NewsDataset(test_df,vocab)
train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True,collate_fn=collate_fn)
val_loader=DataLoader(val_dataset,batch_size=batch_size,shuffle=False,collate_fn=collate_fn)
test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False,collate_fn=collate_fn)
class Encoder(tr.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.embedding=tr.Embedding(vocab_size,embed_size)
        self.lstm=tr.LSTM(embed_size,hidden_size,bidirectional=True)
    def forward(self, x):
        embedded=self.embedding(x)
        outputs,(hidden,cell)=self.lstm(embedded)
        return outputs,hidden,cell
class BahdanauAttention(tr.Module):
    def __init__(self):
        super().__init__()
        self.W1 = tr.Linear(hidden_size * 2, hidden_size)
        self.W2 = tr.Linear(hidden_size, hidden_size)
        self.V = tr.Linear(hidden_size, 1)
    def forward(self, encoder_outputs, decoder_hidden):
        decoder_hidden = decoder_hidden.unsqueeze(1)
        energy = self.V(torch.tanh( self.W1(encoder_outputs) +self.W2(decoder_hidden)))
        attention_weights = torch.softmax(energy, dim=1)
        context = torch.sum(attention_weights * encoder_outputs, dim=1)
        return context

class LuongAttention(tr.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.fc = tr.Linear(hidden_size, hidden_size * 2)
    def forward(self, encoder_outputs, decoder_hidden):
        decoder_hidden = self.fc(decoder_hidden)
        decoder_hidden = decoder_hidden.unsqueeze(2)
        scores = torch.bmm(encoder_outputs, decoder_hidden).squeeze(2)
        attention_weights = torch.softmax(scores, dim=1)
        context = torch.bmm( attention_weights.unsqueeze(1),encoder_outputs).squeeze(1)
        return context

class Decoder(tr.Module):
    def __init__(self, vocab_size, attention_type='bahdanau'):
        super().__init__()
        self.embedding = tr.Embedding(vocab_size, embed_size)
        self.lstm = tr.LSTM(embed_size + hidden_size * 2, hidden_size)
        self.fc = tr.Linear(hidden_size, vocab_size)

        if attention_type == 'bahdanau':
            self.attention = BahdanauAttention()
        else:
            self.attention = LuongAttention(hidden_size)

    def forward(self, x, hidden, cell, encoder_outputs):
        x = x.unsqueeze(0)
        embedded = self.embedding(x)

        context = self.attention(encoder_outputs, hidden[-1])
        if context.dim() == 3:
           context = context.mean(dim=1)
        context = context.unsqueeze(0)


        lstm_input = torch.cat((embedded, context), dim=2)
        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))

        predictions = self.fc(output.squeeze(0))
        return predictions, hidden, cell
import torch
Device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
def train_model(encoder, decoder, loader, optimizer, criterion):
    encoder.train()
    decoder.train()
    total_loss=0
    for article,summary in loader:
        article=article.to(Device).transpose(0, 1)
        summary=summary.to(Device).transpose(0, 1)
        optimizer.zero_grad()
        encoder_outputs,hidden,cell=encoder(article)
        hidden=hidden[0:1]+hidden[1:2]
        cell=cell[0:1]+cell[1:2]
        encoder_outputs=encoder_outputs.permute(1, 0, 2)
        input_token=summary[0]
        loss=0
        for t in range(1,summary.shape[0]):
            output,hidden,cell=decoder(input_token,hidden,cell,encoder_outputs)
            loss +=criterion(output, summary[t])
            input_token=summary[t]
        loss.backward()
        optimizer.step()
        total_loss +=loss.item()
    return total_loss/len(loader)
encoder=Encoder(len(vocab.stoi)).to(Device)
decoder=Decoder(len(vocab.stoi), attention_type='bahdanau').to(Device)
optimizer=tp.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)
criterion=tr.CrossEntropyLoss(ignore_index=vocab.stoi['<pad>'])
for epoch in range(epochs):
    loss=train_model(encoder,decoder,train_loader,optimizer,criterion)
    print(f"Epoch {epoch+1}/{epochs}, Training Loss: {loss:.4f}")
train_df.columns
train_df['article_len'] = train_df['article'].apply(lambda x: len(str(x).split()))
train_df['summary_len'] = train_df['highlights'].apply(lambda x: len(str(x).split()))
train_df[['article_len', 'summary_len']].head()
import matplotlib.pyplot as plt
plt.figure(figsize=(8,5))
plt.hist(train_df['article_len'],bins=50)
plt.title("Distribution of Article Lengths")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")
plt.show()
plt.figure(figsize=(8,5))
plt.hist(train_df['summary_len'], bins=50)
plt.title("Distribution of Summary Lengths")
plt.xlabel("Number of Words")
plt.ylabel("Frequency")
plt.show()
train_losses=[642.133,581.962,550.6647]
epochs_range=range(1,len(train_losses)+1)
plt.figure(figsize=(8,5))
plt.plot(epochs_range,train_losses,marker='o')
plt.xlabel("Epochs")
plt.ylabel("Training Loss")
plt.title("Training Loss vs Epochs")
plt.grid(True)
plt.show()
def generate_summary(article, encoder, decoder, vocab, device, max_len=60):
    encoder.eval()
    decoder.eval()

    tokens = article.lower().split()[:400]

    article_idx = (
        [vocab.stoi['<sos>']] +
        [vocab.stoi.get(tok, vocab.stoi['<unk>']) for tok in tokens] +
        [vocab.stoi['<eos>']]
    )

    article_tensor = torch.LongTensor(article_idx).unsqueeze(1).to(device)
    # (seq_len, 1)

    with torch.no_grad():
        encoder_outputs, hidden, cell = encoder(article_tensor)

        # ðŸ”´ FIX 1: merge BiLSTM states
        hidden = hidden[0:1] + hidden[1:2]
        cell = cell[0:1] + cell[1:2]
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        # (1, seq_len, hidden*2)

    input_token = torch.LongTensor([vocab.stoi['<sos>']]).to(device)
    generated_tokens = []

    for _ in range(max_len):
        with torch.no_grad():
            output, hidden, cell = decoder(
                input_token, hidden, cell, encoder_outputs
            )

        top_token = output.argmax(1).item()

        if top_token == vocab.stoi['<eos>']:
            break

        word = vocab.itos[top_token] if top_token in vocab.itos else "<unk>"
        generated_tokens.append(word)

        input_token = torch.LongTensor([top_token]).to(device)

    return " ".join(generated_tokens)
preds = []
refs = []
Num_Samples = 3
for i in range(Num_Samples):
    article = test_df.iloc[i]['article']
    reference = test_df.iloc[i]['highlights']
    generated = generate_summary( article, encoder, decoder, vocab, Device)
    preds.append(generated)
    refs.append(reference)
    print(f"\nSample {i+1}")
    print("ARTICLE (shortened):")
    print(article[:300], "...")
    print("\nREFERENCE SUMMARY:")
    print(reference)
    print("\nGENERATED SUMMARY:")
    print(generated)
encoder_luong=Encoder(len(vocab.stoi)).to(Device)
decoder_luong=Decoder(len(vocab.stoi), attention_type='luong').to(Device)
optimizer_luong=torch.optim.Adam( list(encoder_luong.parameters())+list(decoder_luong.parameters()),lr=0.001)
for epoch in range(epochs):
    loss = train_model(encoder_luong,decoder_luong,train_loader,optimizer_luong,criterion)
    print(f"Luong Epoch{epoch+1}/{epochs}, Training Loss:{loss:.4f}")
import matplotlib.pyplot as plt
train_losses=[643.8632,595.6941,577.5504]
epochs_range=range(1,len(train_losses)+1)
plt.figure(figsize=(8,5))
plt.plot(epochs_range,train_losses,marker='o')
plt.xlabel("Epochs")
plt.ylabel("Training Loss")
plt.title("Training Loss vs Epochs")
plt.grid(True)
plt.show()
preds_luong=[]
refs_luong=[]
Num_Samples=3
for i in range(Num_Samples):
    article=test_df.iloc[i]['article']
    reference=test_df.iloc[i]['highlights']
    summary=generate_summary( article, encoder_luong,decoder_luong, vocab,Device)
    preds_luong.append(summary)
    refs_luong.append(reference)
    print(f"\nLUONG SAMPLE {i+1}")
    print("REFERENCE:", reference)
    print("GENERATED:", summary)
import pandas as pd
comparison_table = pd.DataFrame({
    'Aspect':['Attention Type','Alignment Method','Training Stability','Training Speed','Summary Quality','Suitability for Long Articles'],
    'Bahdanau Attention':['Additive Attention','Neural network based alignment','More stable','Moderate','Better contextual summaries','High'],
    'Luong Attention':['Multiplicative Attention','Dot-product based alignment','Less stable than Bahdanau','Faster','Reasonable but less expressive','Moderate']})
comparison_table
from rouge_score import rouge_scorer
import pandas as pd
Num_Samp=50
bahdanau_preds=[]
luong_preds=[]
refs=[]
for i in range(Num_Samp):
    article=test_df.iloc[i]['article']
    reference=test_df.iloc[i]['highlights']
    summary_bahdanau=generate_summary(article,encoder,decoder,vocab,Device)
    summary_luong=generate_summary(article,encoder_luong,decoder_luong,vocab,Device)
    bahdanau_preds.append(summary_bahdanau)
    luong_preds.append(summary_luong)
    refs.append(reference)
scorer=rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'],use_stemmer=True)
def compute_rouge(preds,refs):
    r1=r2=rL=0
    for p,r in zip(preds, refs):
        scores = scorer.score(r,p)
        r1 +=scores['rouge1'].fmeasure
        r2 +=scores['rouge2'].fmeasure
        rL +=scores['rougeL'].fmeasure
    n=len(preds)
    return { "ROUGE-1":r1/n,"ROUGE-2":r2/n,"ROUGE-L":rL/n}
rouge_bahdanau=compute_rouge(bahdanau_preds,refs)
rouge_luong=compute_rouge(luong_preds,refs)
print("Bahdanau ROUGE:", rouge_bahdanau)
print("Luong ROUGE:", rouge_luong)
rouge_comparison=pd.DataFrame({
    "Metric":["ROUGE-1","ROUGE-2","ROUGE-L"],
    "Bahdanau Attention":[rouge_bahdanau["ROUGE-1"],rouge_bahdanau["ROUGE-2"], rouge_bahdanau["ROUGE-L"]],
    "Luong Attention": [  rouge_luong["ROUGE-1"], rouge_luong["ROUGE-2"], rouge_luong["ROUGE-L"]]})
rouge_comparison
import matplotlib.pyplot as plt
import numpy as np
data=rouge_comparison.set_index("Metric").T.values
plt.figure(figsize=(7,4))
plt.imshow(data, aspect='auto')
plt.colorbar(label="ROUGE Score")
plt.xticks(range(3), ["ROUGE-1", "ROUGE-2", "ROUGE-L"])
plt.yticks([0, 1], ["Bahdanau Attention", "Luong Attention"])
for i in range(2):
    for j in range(3):
        plt.text(j, i,f"{data[i, j]:.2f}",ha="center", va="center")
plt.title("ROUGE Score Comparison: Bahdanau vs Luong")
plt.show()
